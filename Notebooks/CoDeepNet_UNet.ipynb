{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5efdcce3",
   "metadata": {},
   "source": [
    "# Co-DeepNet U-Net: Cooperative Learning for Medical Image Analysis\n",
    "\n",
    "This notebook implements the Co-DeepNet architecture from \"Co-DeepNet: A Cooperative Convolutional Neural Network\" paper, adapted for medical image segmentation with tumor classification.\n",
    "\n",
    "## Key Concepts:\n",
    "- **Tag-Team Training**: Two U-Nets alternate active training periods\n",
    "- **Knowledge Transmission**: Active network mentors inactive network\n",
    "- **Dual Task**: Classification (tumor present?) + Segmentation (where is tumor?)\n",
    "- **Cooperative Learning**: Networks explore different solution spaces while sharing discoveries\n",
    "\n",
    "## Architecture Overview:\n",
    "```\n",
    "Input ‚Üí [U-Net-A] ‚ü∑ Knowledge Transfer ‚ü∑ [U-Net-B] ‚Üí Ensemble ‚Üí {Classification, Segmentation}\n",
    "         (active)                          (inactive)\n",
    "                    ‚Üì Swap roles ‚Üì\n",
    "         (inactive)                         (active)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccc8433",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b157a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872e71e4",
   "metadata": {},
   "source": [
    "## 2. U-Net Architecture with Classification Head\n",
    "\n",
    "Each U-Net outputs:\n",
    "1. **Segmentation mask**: Pixel-level tumor localization\n",
    "2. **Classification logits**: Binary tumor presence prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58d9698",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(Conv2d ‚Üí BatchNorm ‚Üí ReLU) √ó 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class UNetWithClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net architecture with dual output:\n",
    "    - Segmentation: Pixel-level tumor mask\n",
    "    - Classification: Binary tumor presence\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, seg_classes=1, network_id='A'):\n",
    "        super().__init__()\n",
    "        self.network_id = network_id\n",
    "        \n",
    "        # Encoder (Contracting Path)\n",
    "        self.enc1 = DoubleConv(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc2 = DoubleConv(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc3 = DoubleConv(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc4 = DoubleConv(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(512, 1024)\n",
    "        \n",
    "        # Classification Head (from bottleneck features)\n",
    "        self.clf_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.clf_fc = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1)  # Binary classification\n",
    "        )\n",
    "        \n",
    "        # Decoder (Expanding Path)\n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.dec4 = DoubleConv(1024, 512)\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3 = DoubleConv(512, 256)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = DoubleConv(256, 128)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = DoubleConv(128, 64)\n",
    "        \n",
    "        # Segmentation Output\n",
    "        self.seg_out = nn.Conv2d(64, seg_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(self.pool1(enc1))\n",
    "        enc3 = self.enc3(self.pool2(enc2))\n",
    "        enc4 = self.enc4(self.pool3(enc3))\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "        \n",
    "        # Classification Branch\n",
    "        clf_features = self.clf_pool(bottleneck)\n",
    "        clf_features = clf_features.view(clf_features.size(0), -1)\n",
    "        clf_logits = self.clf_fc(clf_features)\n",
    "        \n",
    "        # Decoder\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat([dec4, enc4], dim=1)\n",
    "        dec4 = self.dec4(dec4)\n",
    "        \n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat([dec3, enc3], dim=1)\n",
    "        dec3 = self.dec3(dec3)\n",
    "        \n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat([dec2, enc2], dim=1)\n",
    "        dec2 = self.dec2(dec2)\n",
    "        \n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat([dec1, enc1], dim=1)\n",
    "        dec1 = self.dec1(dec1)\n",
    "        \n",
    "        # Segmentation Output\n",
    "        seg_logits = self.seg_out(dec1)\n",
    "        \n",
    "        return seg_logits, clf_logits\n",
    "    \n",
    "    def get_encoder_params(self):\n",
    "        \"\"\"Get encoder parameters for knowledge transfer\"\"\"\n",
    "        return list(self.enc1.parameters()) + list(self.enc2.parameters()) + \\\n",
    "               list(self.enc3.parameters()) + list(self.enc4.parameters())\n",
    "    \n",
    "    def get_decoder_params(self):\n",
    "        \"\"\"Get decoder parameters\"\"\"\n",
    "        return list(self.dec1.parameters()) + list(self.dec2.parameters()) + \\\n",
    "               list(self.dec3.parameters()) + list(self.dec4.parameters())\n",
    "    \n",
    "    def get_bottleneck_params(self):\n",
    "        \"\"\"Get bottleneck parameters\"\"\"\n",
    "        return list(self.bottleneck.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d87e901",
   "metadata": {},
   "source": [
    "## 3. Knowledge Transmission Module\n",
    "\n",
    "Implements the mentoring mechanism where the active network transfers learned knowledge to the inactive network.\n",
    "\n",
    "**Transmission Strategies:**\n",
    "- `encoder_only`: Transfer early feature extractors (like Co-DeepNet paper)\n",
    "- `full_network`: Transfer all parameters\n",
    "- `bottleneck_only`: Transfer only the deepest representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1d4127",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeTransmitter:\n",
    "    \"\"\"\n",
    "    Implements knowledge transmission from active to inactive network.\n",
    "    \n",
    "    Based on Co-DeepNet paper: the active network mentors the inactive one,\n",
    "    allowing the inactive network to benefit from discoveries without training.\n",
    "    \"\"\"\n",
    "    def __init__(self, transmission_rate=0.3, strategy='encoder_only'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            transmission_rate: Weight for knowledge transfer (0.3 = 30% from active, 70% keep own)\n",
    "            strategy: 'encoder_only', 'full_network', or 'bottleneck_only'\n",
    "        \"\"\"\n",
    "        self.transmission_rate = transmission_rate\n",
    "        self.strategy = strategy\n",
    "        self.transmission_count = 0\n",
    "    \n",
    "    def transmit(self, active_net: UNetWithClassifier, inactive_net: UNetWithClassifier):\n",
    "        \"\"\"\n",
    "        Transfer knowledge from active to inactive network.\n",
    "        \n",
    "        Formula: Œ∏_inactive = (1-Œ±) * Œ∏_inactive + Œ± * Œ∏_active\n",
    "        where Œ± is the transmission_rate\n",
    "        \"\"\"\n",
    "        self.transmission_count += 1\n",
    "        alpha = self.transmission_rate\n",
    "        \n",
    "        # Select parameters based on strategy\n",
    "        if self.strategy == 'encoder_only':\n",
    "            active_params = active_net.get_encoder_params()\n",
    "            inactive_params = inactive_net.get_encoder_params()\n",
    "        elif self.strategy == 'bottleneck_only':\n",
    "            active_params = active_net.get_bottleneck_params()\n",
    "            inactive_params = inactive_net.get_bottleneck_params()\n",
    "        elif self.strategy == 'full_network':\n",
    "            active_params = list(active_net.parameters())\n",
    "            inactive_params = list(inactive_net.parameters())\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {self.strategy}\")\n",
    "        \n",
    "        # Transfer knowledge with EMA-like update\n",
    "        with torch.no_grad():\n",
    "            for inactive_param, active_param in zip(inactive_params, active_params):\n",
    "                # Inactive retains most of its knowledge but learns from active\n",
    "                inactive_param.data = (1 - alpha) * inactive_param.data + alpha * active_param.data\n",
    "        \n",
    "        return {\n",
    "            'transmission_id': self.transmission_count,\n",
    "            'strategy': self.strategy,\n",
    "            'rate': alpha,\n",
    "            'params_transferred': len(list(active_params))\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfe8057",
   "metadata": {},
   "source": [
    "## 4. Co-DeepNet Training Manager\n",
    "\n",
    "Implements the tag-team training strategy:\n",
    "1. **One network trains** while the other rests\n",
    "2. **Periodic knowledge transmission** from active to inactive\n",
    "3. **Role swap** at specified intervals\n",
    "4. **Ensemble prediction** for final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39220c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoDeepNetTrainer:\n",
    "    \"\"\"\n",
    "    Manages cooperative training of two U-Nets following Co-DeepNet principles.\n",
    "    \n",
    "    Key mechanisms:\n",
    "    - Tag-team: Only one network trains at a time\n",
    "    - Knowledge transmission: Active mentors inactive periodically\n",
    "    - Role swapping: Networks alternate being active\n",
    "    - Ensemble: Average both outputs for final prediction\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        unet_a: UNetWithClassifier,\n",
    "        unet_b: UNetWithClassifier,\n",
    "        optimizer_a: torch.optim.Optimizer,\n",
    "        optimizer_b: torch.optim.Optimizer,\n",
    "        swap_frequency: int = 50,  # Swap roles every N batches\n",
    "        transmission_frequency: int = 10,  # Transmit knowledge every N batches\n",
    "        transmission_rate: float = 0.3,\n",
    "        transmission_strategy: str = 'encoder_only',\n",
    "        seg_weight: float = 1.0,\n",
    "        clf_weight: float = 0.5,\n",
    "        device: str = 'cuda'\n",
    "    ):\n",
    "        self.unet_a = unet_a.to(device)\n",
    "        self.unet_b = unet_b.to(device)\n",
    "        self.optimizer_a = optimizer_a\n",
    "        self.optimizer_b = optimizer_b\n",
    "        \n",
    "        self.swap_frequency = swap_frequency\n",
    "        self.transmission_frequency = transmission_frequency\n",
    "        self.seg_weight = seg_weight\n",
    "        self.clf_weight = clf_weight\n",
    "        self.device = device\n",
    "        \n",
    "        # Knowledge transmitter\n",
    "        self.transmitter = KnowledgeTransmitter(\n",
    "            transmission_rate=transmission_rate,\n",
    "            strategy=transmission_strategy\n",
    "        )\n",
    "        \n",
    "        # Training state\n",
    "        self.active_network = 'A'  # Start with network A\n",
    "        self.batch_count = 0\n",
    "        self.swap_count = 0\n",
    "        \n",
    "        # Loss functions\n",
    "        self.seg_criterion = nn.BCEWithLogitsLoss()\n",
    "        self.clf_criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        # Metrics tracking\n",
    "        self.history = {\n",
    "            'network_a': {'total_loss': [], 'seg_loss': [], 'clf_loss': []},\n",
    "            'network_b': {'total_loss': [], 'seg_loss': [], 'clf_loss': []},\n",
    "            'active_network': [],\n",
    "            'transmissions': []\n",
    "        }\n",
    "    \n",
    "    def _compute_loss(\n",
    "        self, \n",
    "        seg_logits: torch.Tensor, \n",
    "        clf_logits: torch.Tensor,\n",
    "        seg_target: torch.Tensor,\n",
    "        clf_target: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Compute combined segmentation + classification loss\"\"\"\n",
    "        seg_loss = self.seg_criterion(seg_logits, seg_target)\n",
    "        clf_loss = self.clf_criterion(clf_logits.squeeze(), clf_target.float())\n",
    "        \n",
    "        total_loss = self.seg_weight * seg_loss + self.clf_weight * clf_loss\n",
    "        return total_loss, seg_loss, clf_loss\n",
    "    \n",
    "    def train_step(self, images: torch.Tensor, seg_masks: torch.Tensor, clf_labels: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Single training step following Co-DeepNet protocol.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with losses and metadata\n",
    "        \"\"\"\n",
    "        self.batch_count += 1\n",
    "        images = images.to(self.device)\n",
    "        seg_masks = seg_masks.to(self.device)\n",
    "        clf_labels = clf_labels.to(self.device)\n",
    "        \n",
    "        # Determine active network\n",
    "        if self.active_network == 'A':\n",
    "            active_net = self.unet_a\n",
    "            active_optimizer = self.optimizer_a\n",
    "            inactive_net = self.unet_b\n",
    "        else:\n",
    "            active_net = self.unet_b\n",
    "            active_optimizer = self.optimizer_b\n",
    "            inactive_net = self.unet_a\n",
    "        \n",
    "        # Train ONLY active network (tag-team principle)\n",
    "        active_net.train()\n",
    "        inactive_net.eval()  # Inactive network rests\n",
    "        \n",
    "        active_optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass (active network only)\n",
    "        seg_logits, clf_logits = active_net(images)\n",
    "        \n",
    "        # Compute loss\n",
    "        total_loss, seg_loss, clf_loss = self._compute_loss(\n",
    "            seg_logits, clf_logits, seg_masks, clf_labels\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        active_optimizer.step()\n",
    "        \n",
    "        # Knowledge transmission (periodic mentoring)\n",
    "        transmission_info = None\n",
    "        if self.batch_count % self.transmission_frequency == 0:\n",
    "            transmission_info = self.transmitter.transmit(active_net, inactive_net)\n",
    "            self.history['transmissions'].append({\n",
    "                'batch': self.batch_count,\n",
    "                'from': self.active_network,\n",
    "                'info': transmission_info\n",
    "            })\n",
    "        \n",
    "        # Role swap (tag-team rotation)\n",
    "        swapped = False\n",
    "        if self.batch_count % self.swap_frequency == 0:\n",
    "            self.active_network = 'B' if self.active_network == 'A' else 'A'\n",
    "            self.swap_count += 1\n",
    "            swapped = True\n",
    "        \n",
    "        # Record metrics\n",
    "        net_key = 'network_a' if self.active_network == 'A' else 'network_b'\n",
    "        self.history[net_key]['total_loss'].append(total_loss.item())\n",
    "        self.history[net_key]['seg_loss'].append(seg_loss.item())\n",
    "        self.history[net_key]['clf_loss'].append(clf_loss.item())\n",
    "        self.history['active_network'].append(self.active_network)\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss.item(),\n",
    "            'seg_loss': seg_loss.item(),\n",
    "            'clf_loss': clf_loss.item(),\n",
    "            'active_network': self.active_network,\n",
    "            'swapped': swapped,\n",
    "            'transmission': transmission_info is not None\n",
    "        }\n",
    "    \n",
    "    def predict_ensemble(self, images: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Ensemble prediction: Average outputs from both networks.\n",
    "        \n",
    "        This is the key benefit of cooperative learning - two networks\n",
    "        that explored different solution spaces combine their knowledge.\n",
    "        \"\"\"\n",
    "        self.unet_a.eval()\n",
    "        self.unet_b.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            images = images.to(self.device)\n",
    "            \n",
    "            # Get predictions from both networks\n",
    "            seg_a, clf_a = self.unet_a(images)\n",
    "            seg_b, clf_b = self.unet_b(images)\n",
    "            \n",
    "            # Ensemble: Average logits\n",
    "            seg_ensemble = (seg_a + seg_b) / 2\n",
    "            clf_ensemble = (clf_a + clf_b) / 2\n",
    "            \n",
    "            # Apply sigmoid for probabilities\n",
    "            seg_probs = torch.sigmoid(seg_ensemble)\n",
    "            clf_probs = torch.sigmoid(clf_ensemble)\n",
    "        \n",
    "        return seg_probs, clf_probs, (seg_a, clf_a), (seg_b, clf_b)\n",
    "    \n",
    "    def get_training_summary(self):\n",
    "        \"\"\"Get summary of training dynamics\"\"\"\n",
    "        return {\n",
    "            'total_batches': self.batch_count,\n",
    "            'total_swaps': self.swap_count,\n",
    "            'total_transmissions': self.transmitter.transmission_count,\n",
    "            'final_active': self.active_network,\n",
    "            'avg_loss_a': np.mean(self.history['network_a']['total_loss'][-100:]) if self.history['network_a']['total_loss'] else 0,\n",
    "            'avg_loss_b': np.mean(self.history['network_b']['total_loss'][-100:]) if self.history['network_b']['total_loss'] else 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f3d843",
   "metadata": {},
   "source": [
    "## 5. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef1b4dc",
   "metadata": {},
   "source": [
    "## 5a. Data Directory Explorer (Debug)\n",
    "\n",
    "Let's first verify the actual structure and find the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de96acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the correct data directory\n",
    "import os\n",
    "\n",
    "# Get current working directory\n",
    "cwd = Path.cwd()\n",
    "print(f\"Current working directory: {cwd}\")\n",
    "\n",
    "# Project root (should be DeepLearning-MiniProject)\n",
    "project_root = Path('/Users/idahayjorgensen/Documents/cs/deep_learning/DeepLearning-MiniProject')\n",
    "print(f\"\\nProject root: {project_root}\")\n",
    "print(f\"Project root exists: {project_root.exists()}\")\n",
    "\n",
    "# Check for augmented_data\n",
    "augmented_data_path = project_root / 'augmented_data'\n",
    "print(f\"\\nAugmented data path: {augmented_data_path}\")\n",
    "print(f\"Augmented data exists: {augmented_data_path.exists()}\")\n",
    "\n",
    "if augmented_data_path.exists():\n",
    "    print(\"\\nüìÇ Directory structure:\")\n",
    "    for item in sorted(augmented_data_path.rglob('*'))[:20]:  # Show first 20 items\n",
    "        relative = item.relative_to(augmented_data_path)\n",
    "        if item.is_dir():\n",
    "            print(f\"  üìÅ {relative}/\")\n",
    "        else:\n",
    "            print(f\"  üìÑ {relative}\")\n",
    "    \n",
    "    # Count files\n",
    "    control_imgs = list((augmented_data_path / 'controls' / 'imgs').glob('*.*')) if (augmented_data_path / 'controls' / 'imgs').exists() else []\n",
    "    patient_imgs = list((augmented_data_path / 'patients' / 'imgs').glob('*.*')) if (augmented_data_path / 'patients' / 'imgs').exists() else []\n",
    "    patient_labels = list((augmented_data_path / 'patients' / 'labels').glob('*.*')) if (augmented_data_path / 'patients' / 'labels').exists() else []\n",
    "    \n",
    "    print(f\"\\nüìä File counts:\")\n",
    "    print(f\"  Controls: {len(control_imgs)}\")\n",
    "    print(f\"  Patient images: {len(patient_imgs)}\")\n",
    "    print(f\"  Patient labels: {len(patient_labels)}\")\n",
    "    \n",
    "    # Show sample filenames\n",
    "    if control_imgs:\n",
    "        print(f\"\\n  Sample control files: {[f.name for f in control_imgs[:3]]}\")\n",
    "    if patient_imgs:\n",
    "        print(f\"  Sample patient files: {[f.name for f in patient_imgs[:3]]}\")\n",
    "    if patient_labels:\n",
    "        print(f\"  Sample label files: {[f.name for f in patient_labels[:3]]}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Augmented data directory not found!\")\n",
    "    print(\"Searching for it...\")\n",
    "    \n",
    "    # Search for augmented_data in common locations\n",
    "    search_paths = [\n",
    "        project_root / 'augmented_data',\n",
    "        project_root / 'data' / 'augmented_data',\n",
    "        project_root / 'Notebooks' / 'augmented_data',\n",
    "        Path.cwd() / 'augmented_data',\n",
    "    ]\n",
    "    \n",
    "    for search_path in search_paths:\n",
    "        print(f\"  Checking: {search_path} ... {search_path.exists()}\")\n",
    "        if search_path.exists():\n",
    "            print(f\"  ‚úì Found it!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003e1059",
   "metadata": {},
   "source": [
    "## 5b. Load Augmented Dataset üéØ\n",
    "\n",
    "### ‚úÖ Your Augmented Data is Ready!\n",
    "\n",
    "From your EDA notebook, you created an augmented dataset with:\n",
    "- **Controls**: ~426 images (healthy, no tumors)\n",
    "- **Patients**: ~182 √ó 5 = ~910 images (with augmentations: original + horizontal_flip + vertical_flip + rotate_90 + rotate_180)\n",
    "- **Total**: ~1,336 images with proper class balance!\n",
    "\n",
    "The augmented data is located at:\n",
    "```\n",
    "/Users/idahayjorgensen/Documents/cs/deep_learning/DeepLearning-MiniProject/augmented_data/\n",
    "‚îú‚îÄ‚îÄ controls/imgs/          # .png files\n",
    "‚îî‚îÄ‚îÄ patients/\n",
    "    ‚îú‚îÄ‚îÄ imgs/               # patient_XXX_augtype.png\n",
    "    ‚îî‚îÄ‚îÄ labels/             # segmentation_XXX_augtype.png\n",
    "```\n",
    "\n",
    "The cell below will:\n",
    "1. Auto-detect the correct data path (prioritizes local augmented data)\n",
    "2. Smart matching: `patient_000_horizontal_flip.png` ‚Üí `segmentation_000_horizontal_flip.png`\n",
    "3. Load BOTH controls and patients for proper training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5cfafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class TumorSegmentationDataset(Dataset):\n",
    "    \"\"\"Dataset for MIP-PET tumor images with segmentation masks and classification labels\"\"\"\n",
    "    def __init__(self, data_dir: str, split='train', include_controls=True, include_patients=True):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        \n",
    "        print(f\"Loading dataset from: {self.data_dir}\")\n",
    "        print(f\"Directory exists: {self.data_dir.exists()}\")\n",
    "        \n",
    "        self.samples = []\n",
    "        \n",
    "        # ===== LOAD CONTROLS (no tumor) =====\n",
    "        if include_controls:\n",
    "            control_dir = self.data_dir / 'controls' / 'imgs'\n",
    "            print(f\"\\nüìÇ Looking for controls in: {control_dir}\")\n",
    "            print(f\"   Control directory exists: {control_dir.exists()}\")\n",
    "            \n",
    "            if control_dir.exists():\n",
    "                # Look for .png files\n",
    "                self.control_files = sorted(list(control_dir.glob('*.png')))\n",
    "                print(f\"   Found {len(self.control_files)} control PNG files\")\n",
    "                if self.control_files:\n",
    "                    print(f\"   Sample: {[f.name for f in self.control_files[:3]]}\")\n",
    "                \n",
    "                # Add controls (no segmentation mask, just zeros)\n",
    "                for img_path in self.control_files:\n",
    "                    self.samples.append((img_path, None, 0))  # 0 = no tumor\n",
    "            else:\n",
    "                self.control_files = []\n",
    "                print(f\"   ‚ö†Ô∏è Control directory not found!\")\n",
    "        else:\n",
    "            self.control_files = []\n",
    "            print(f\"\\n‚è≠Ô∏è  Skipping controls (include_controls=False)\")\n",
    "        \n",
    "        # ===== LOAD PATIENTS (with tumors) =====\n",
    "        if include_patients:\n",
    "            patient_img_dir = self.data_dir / 'patients' / 'imgs'\n",
    "            patient_label_dir = self.data_dir / 'patients' / 'labels'\n",
    "            \n",
    "            print(f\"\\nüìÇ Looking for patients in: {patient_img_dir}\")\n",
    "            print(f\"   Patient directory exists: {patient_img_dir.exists()}\")\n",
    "            print(f\"   Label directory exists: {patient_label_dir.exists()}\")\n",
    "            \n",
    "            if patient_img_dir.exists():\n",
    "                # Look for .png files\n",
    "                self.patient_files = sorted(list(patient_img_dir.glob('*.png')))\n",
    "                print(f\"   Found {len(self.patient_files)} patient PNG files\")\n",
    "                \n",
    "                if self.patient_files:\n",
    "                    print(f\"   Sample: {[f.name for f in self.patient_files[:3]]}\")\n",
    "                \n",
    "                # Add patients (with segmentation masks)\n",
    "                missing_labels = []\n",
    "                found_labels = 0\n",
    "                \n",
    "                for img_path in self.patient_files:\n",
    "                    # Smart label matching: patient_XXX_augtype.png ‚Üí segmentation_XXX_augtype.png\n",
    "                    img_stem = img_path.stem  # e.g., \"patient_000_horizontal_flip\"\n",
    "                    \n",
    "                    # Try multiple label naming patterns (PNG)\n",
    "                    possible_label_names = [\n",
    "                        img_path.name,  # Same name (patient_XXX.png)\n",
    "                        img_stem + '.png',  # Same stem with .png\n",
    "                        img_stem.replace('patient_', 'segmentation_') + '.png',\n",
    "                    ]\n",
    "                    \n",
    "                    label_path = None\n",
    "                    for label_name in possible_label_names:\n",
    "                        test_path = patient_label_dir / label_name\n",
    "                        if test_path.exists():\n",
    "                            label_path = test_path\n",
    "                            break\n",
    "                    \n",
    "                    if label_path:\n",
    "                        self.samples.append((img_path, label_path, 1))  # 1 = tumor present\n",
    "                        found_labels += 1\n",
    "                    else:\n",
    "                        missing_labels.append(img_path.name)\n",
    "                \n",
    "                print(f\"   ‚úì Matched {found_labels} patient images with labels\")\n",
    "                \n",
    "                if missing_labels:\n",
    "                    print(f\"   ‚ö†Ô∏è Warning: {len(missing_labels)} patient images missing labels\")\n",
    "                    if len(missing_labels) <= 5:\n",
    "                        print(f\"   Missing labels for: {missing_labels}\")\n",
    "            else:\n",
    "                self.patient_files = []\n",
    "                print(f\"   ‚ö†Ô∏è Patient directory not found!\")\n",
    "        else:\n",
    "            self.patient_files = []\n",
    "            print(f\"\\n‚è≠Ô∏è  Skipping patients (include_patients=False)\")\n",
    "        \n",
    "        # ===== SUMMARY =====\n",
    "        num_controls = len([s for s in self.samples if s[2] == 0])\n",
    "        num_patients = len([s for s in self.samples if s[2] == 1])\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìä DATASET SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"   Controls (no tumor):  {num_controls} samples\")\n",
    "        print(f\"   Patients (w/ tumor):  {num_patients} samples\")\n",
    "        print(f\"   Total samples:        {len(self.samples)} samples\")\n",
    "        print(f\"   Class balance:        {num_patients}/{num_controls+num_patients} = {num_patients/(num_controls+num_patients)*100:.1f}% positive\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        if len(self.samples) == 0:\n",
    "            print(\"\\n‚ùå ERROR: No samples found!\")\n",
    "            print(f\"Please check that PNG image files exist in:\")\n",
    "            print(f\"  - {self.data_dir / 'controls' / 'imgs'}\")\n",
    "            print(f\"  - {self.data_dir / 'patients' / 'imgs'}\")\n",
    "            raise ValueError(f\"No PNG data found in {self.data_dir}. Please check directory structure.\")\n",
    "        \n",
    "        if num_patients == 0:\n",
    "            print(\"\\n‚ö†Ô∏è  WARNING: No patient samples with tumors found!\")\n",
    "            print(\"   The model will not learn to detect tumors.\")\n",
    "            print(\"   Check that patient images and labels are in the correct directory.\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def _load_image(self, path: Path) -> np.ndarray:\n",
    "        \"\"\"Load PNG image and convert to grayscale numpy array\"\"\"\n",
    "        img = Image.open(path).convert('L')\n",
    "        return np.array(img)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label_path, has_tumor = self.samples[idx]\n",
    "        \n",
    "        # Load image (PNG)\n",
    "        image = self._load_image(img_path)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        # Load segmentation mask (or create empty for controls)\n",
    "        if label_path and label_path.exists():\n",
    "            mask = self._load_image(label_path)\n",
    "            if mask.max() > 1.0:\n",
    "                mask = mask / 255.0\n",
    "        else:\n",
    "            mask = np.zeros_like(image)\n",
    "        \n",
    "        # Add channel dimension if needed\n",
    "        if image.ndim == 2:\n",
    "            image = image[np.newaxis, ...]\n",
    "        if mask.ndim == 2:\n",
    "            mask = mask[np.newaxis, ...]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        image = torch.from_numpy(image).float()\n",
    "        mask = torch.from_numpy(mask).float()\n",
    "        clf_label = torch.tensor(has_tumor, dtype=torch.long)\n",
    "        \n",
    "        return image, mask, clf_label\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Create Dataset - Try Multiple Possible Paths\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"üîç INITIALIZING MIP-PET TUMOR SEGMENTATION DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Try different possible data paths (prioritize local augmented data!)\n",
    "possible_paths = [\n",
    "    Path('/Users/idahayjorgensen/Documents/cs/deep_learning/DeepLearning-MiniProject/augmented_data'),  # ‚úÖ LOCAL AUGMENTED (BEST!)\n",
    "    Path('/Users/idahayjorgensen/Documents/cs/deep_learning/DeepLearning-MiniProject/data'),  # Local original\n",
    "    Path('/work/IdaHayJ√∏rgensen#9284/Notebooks/augmented_data'),  # Virtual env augmented\n",
    "    Path('/work/IdaHayJ√∏rgensen#9284/Notebooks/data'),  # Virtual env original\n",
    "]\n",
    "\n",
    "DATA_DIR = None\n",
    "for path in possible_paths:\n",
    "    print(f\"\\nüîç Checking: {path}\")\n",
    "    if path.exists():\n",
    "        # Check if it has the right structure\n",
    "        has_controls = (path / 'controls' / 'imgs').exists()\n",
    "        has_patients = (path / 'patients' / 'imgs').exists()\n",
    "        \n",
    "        print(f\"   Controls found: {has_controls}\")\n",
    "        print(f\"   Patients found: {has_patients}\")\n",
    "        \n",
    "        if has_controls or has_patients:\n",
    "            DATA_DIR = path\n",
    "            print(f\"   ‚úÖ Using this directory!\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"   ‚ùå Directory doesn't exist\")\n",
    "\n",
    "if DATA_DIR is None:\n",
    "    print(\"\\n‚ùå ERROR: Could not find data directory!\")\n",
    "    print(\"\\nPlease verify your data is in one of these locations:\")\n",
    "    for path in possible_paths:\n",
    "        print(f\"  ‚Ä¢ {path}\")\n",
    "    raise FileNotFoundError(\"Data directory not found\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üìÇ Final data directory: {DATA_DIR}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Create dataset with BOTH controls and patients\n",
    "train_dataset = TumorSegmentationDataset(\n",
    "    str(DATA_DIR),\n",
    "    include_controls=True,  # Include healthy controls (no tumors)\n",
    "    include_patients=True   # Include patients with tumors\n",
    ")\n",
    "\n",
    "# Create dataloaders only if we have data\n",
    "if len(train_dataset) > 0:\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    print(f\"‚úì Training batches: {len(train_loader)}\")\n",
    "    print(f\"‚úì Batch size: 8\")\n",
    "    print(f\"‚úì Total samples per epoch: {len(train_dataset)}\")\n",
    "    \n",
    "    # Show class distribution in first batch\n",
    "    first_batch = next(iter(train_loader))\n",
    "    images, masks, labels = first_batch\n",
    "    print(f\"\\nüìä First batch statistics:\")\n",
    "    print(f\"   Image shape: {images.shape}\")\n",
    "    print(f\"   Mask shape: {masks.shape}\")\n",
    "    print(f\"   Labels: {labels.tolist()} (0=no tumor, 1=tumor)\")\n",
    "    print(f\"   Tumors in batch: {labels.sum().item()}/{len(labels)}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Cannot create DataLoader - no samples in dataset!\")\n",
    "    print(\"Please verify your data directory structure.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c90621d",
   "metadata": {},
   "source": [
    "## 6. Initialize Co-DeepNet System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e077a438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize two U-Nets with different random initializations\n",
    "# This ensures they explore different solution spaces\n",
    "set_seed(42)\n",
    "unet_a = UNetWithClassifier(in_channels=1, seg_classes=1, network_id='A')\n",
    "\n",
    "set_seed(123)  # Different seed for diversity\n",
    "unet_b = UNetWithClassifier(in_channels=1, seg_classes=1, network_id='B')\n",
    "\n",
    "# Separate optimizers for each network\n",
    "optimizer_a = torch.optim.Adam(unet_a.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "optimizer_b = torch.optim.Adam(unet_b.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# Initialize Co-DeepNet trainer\n",
    "codeepnet_trainer = CoDeepNetTrainer(\n",
    "    unet_a=unet_a,\n",
    "    unet_b=unet_b,\n",
    "    optimizer_a=optimizer_a,\n",
    "    optimizer_b=optimizer_b,\n",
    "    swap_frequency=50,  # Swap active network every 50 batches\n",
    "    transmission_frequency=10,  # Transmit knowledge every 10 batches\n",
    "    transmission_rate=0.3,  # 30% from active, 70% keep own (like paper)\n",
    "    transmission_strategy='encoder_only',  # Transfer early features\n",
    "    seg_weight=1.0,\n",
    "    clf_weight=0.5,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Co-DeepNet initialized with:\")\n",
    "print(f\"  - Tag-team frequency: {codeepnet_trainer.swap_frequency} batches\")\n",
    "print(f\"  - Knowledge transmission: every {codeepnet_trainer.transmission_frequency} batches\")\n",
    "print(f\"  - Transmission rate: {codeepnet_trainer.transmitter.transmission_rate}\")\n",
    "print(f\"  - Strategy: {codeepnet_trainer.transmitter.strategy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b255b4f",
   "metadata": {},
   "source": [
    "## 7. Training Loop ‚ö° **RUN THIS FIRST!**\n",
    "\n",
    "**‚ö†Ô∏è MUST RUN THIS BEFORE EVALUATION!**\n",
    "\n",
    "This cell trains the Co-DeepNet model. Without training, all metrics will be zero.\n",
    "\n",
    "Implements the full Co-DeepNet training protocol:\n",
    "- One network trains (active) while the other rests (inactive)\n",
    "- Periodic knowledge transmission from active to inactive\n",
    "- Regular role swaps to ensure both networks contribute\n",
    "\n",
    "**Expected time**: ~5-15 minutes depending on hardware (20 epochs √ó 54 batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe3da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(trainer: CoDeepNetTrainer, dataloader: DataLoader, epoch: int):\n",
    "    \"\"\"Train for one epoch using Co-DeepNet protocol\"\"\"\n",
    "    epoch_metrics = {\n",
    "        'total_loss': [],\n",
    "        'seg_loss': [],\n",
    "        'clf_loss': [],\n",
    "        'swaps': 0,\n",
    "        'transmissions': 0\n",
    "    }\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\")\n",
    "    for images, masks, clf_labels in pbar:\n",
    "        # Single training step (tag-team)\n",
    "        metrics = trainer.train_step(images, masks, clf_labels)\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        epoch_metrics['total_loss'].append(metrics['total_loss'])\n",
    "        epoch_metrics['seg_loss'].append(metrics['seg_loss'])\n",
    "        epoch_metrics['clf_loss'].append(metrics['clf_loss'])\n",
    "        if metrics['swapped']:\n",
    "            epoch_metrics['swaps'] += 1\n",
    "        if metrics['transmission']:\n",
    "            epoch_metrics['transmissions'] += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{metrics['total_loss']:.4f}\",\n",
    "            'active': metrics['active_network'],\n",
    "            'swaps': epoch_metrics['swaps'],\n",
    "            'tx': epoch_metrics['transmissions']\n",
    "        })\n",
    "    \n",
    "    # Compute epoch averages\n",
    "    return {\n",
    "        'avg_total_loss': np.mean(epoch_metrics['total_loss']),\n",
    "        'avg_seg_loss': np.mean(epoch_metrics['seg_loss']),\n",
    "        'avg_clf_loss': np.mean(epoch_metrics['clf_loss']),\n",
    "        'swaps': epoch_metrics['swaps'],\n",
    "        'transmissions': epoch_metrics['transmissions']\n",
    "    }\n",
    "\n",
    "\n",
    "# Training configuration\n",
    "NUM_EPOCHS = 20\n",
    "training_history = []\n",
    "\n",
    "print(\"\\nüöÄ Starting Co-DeepNet Training...\\n\")\n",
    "print(\"Tag-team protocol:\")\n",
    "print(\"  ‚Üí One network trains, one rests\")\n",
    "print(\"  ‚Üí Active mentors inactive periodically\")\n",
    "print(\"  ‚Üí Networks swap roles regularly\\n\")\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_metrics = train_epoch(codeepnet_trainer, train_loader, epoch)\n",
    "    training_history.append(epoch_metrics)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}:\")\n",
    "    print(f\"  Total Loss: {epoch_metrics['avg_total_loss']:.4f}\")\n",
    "    print(f\"  Seg Loss: {epoch_metrics['avg_seg_loss']:.4f}\")\n",
    "    print(f\"  Clf Loss: {epoch_metrics['avg_clf_loss']:.4f}\")\n",
    "    print(f\"  Network Swaps: {epoch_metrics['swaps']}\")\n",
    "    print(f\"  Knowledge Transmissions: {epoch_metrics['transmissions']}\")\n",
    "\n",
    "# Training summary\n",
    "summary = codeepnet_trainer.get_training_summary()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéì Training Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total batches processed: {summary['total_batches']}\")\n",
    "print(f\"Total network swaps: {summary['total_swaps']}\")\n",
    "print(f\"Total knowledge transmissions: {summary['total_transmissions']}\")\n",
    "print(f\"Final active network: {summary['final_active']}\")\n",
    "print(f\"Network A final loss: {summary['avg_loss_a']:.4f}\")\n",
    "print(f\"Network B final loss: {summary['avg_loss_b']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2829cae1",
   "metadata": {},
   "source": [
    "## 8. Visualization: Training Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e254aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_dynamics(trainer: CoDeepNetTrainer):\n",
    "    \"\"\"Visualize Co-DeepNet training dynamics\"\"\"\n",
    "    history = trainer.history\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Loss comparison\n",
    "    ax = axes[0, 0]\n",
    "    if history['network_a']['total_loss']:\n",
    "        ax.plot(history['network_a']['total_loss'], label='Network A', alpha=0.7)\n",
    "    if history['network_b']['total_loss']:\n",
    "        ax.plot(history['network_b']['total_loss'], label='Network B', alpha=0.7)\n",
    "    ax.set_xlabel('Batch')\n",
    "    ax.set_ylabel('Total Loss')\n",
    "    ax.set_title('Loss Trajectories (Tag-Team Training)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Active network timeline\n",
    "    ax = axes[0, 1]\n",
    "    active_encoded = [1 if net == 'A' else 2 for net in history['active_network']]\n",
    "    ax.plot(active_encoded, linewidth=2)\n",
    "    ax.set_xlabel('Batch')\n",
    "    ax.set_ylabel('Active Network')\n",
    "    ax.set_yticks([1, 2])\n",
    "    ax.set_yticklabels(['Network A', 'Network B'])\n",
    "    ax.set_title('Network Role Swaps Over Time')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Segmentation vs Classification loss\n",
    "    ax = axes[1, 0]\n",
    "    all_seg = history['network_a']['seg_loss'] + history['network_b']['seg_loss']\n",
    "    all_clf = history['network_a']['clf_loss'] + history['network_b']['clf_loss']\n",
    "    if all_seg:\n",
    "        ax.plot(all_seg, label='Segmentation', alpha=0.7)\n",
    "    if all_clf:\n",
    "        ax.plot(all_clf, label='Classification', alpha=0.7)\n",
    "    ax.set_xlabel('Batch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Task-Specific Losses')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Knowledge transmission events\n",
    "    ax = axes[1, 1]\n",
    "    if history['transmissions']:\n",
    "        tx_batches = [t['batch'] for t in history['transmissions']]\n",
    "        tx_heights = [1] * len(tx_batches)\n",
    "        ax.stem(tx_batches, tx_heights, basefmt=' ')\n",
    "        ax.set_xlabel('Batch')\n",
    "        ax.set_ylabel('Transmission Event')\n",
    "        ax.set_title(f'Knowledge Transmissions (n={len(tx_batches)})')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_dynamics(codeepnet_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b81495",
   "metadata": {},
   "source": [
    "## 9. Ensemble Prediction & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sample(trainer: CoDeepNetTrainer, dataloader: DataLoader, num_samples=4):\n",
    "    \"\"\"Visualize ensemble predictions vs individual networks\"\"\"\n",
    "    trainer.unet_a.eval()\n",
    "    trainer.unet_b.eval()\n",
    "    \n",
    "    # Get a batch\n",
    "    images, masks, clf_labels = next(iter(dataloader))\n",
    "    images = images[:num_samples]\n",
    "    masks = masks[:num_samples]\n",
    "    clf_labels = clf_labels[:num_samples]\n",
    "    \n",
    "    # Get predictions\n",
    "    seg_ensemble, clf_ensemble, (seg_a, clf_a), (seg_b, clf_b) = trainer.predict_ensemble(images)\n",
    "    \n",
    "    # Move to CPU for visualization\n",
    "    images = images.cpu()\n",
    "    masks = masks.cpu()\n",
    "    seg_ensemble = seg_ensemble.cpu()\n",
    "    seg_a = torch.sigmoid(seg_a).cpu()\n",
    "    seg_b = torch.sigmoid(seg_b).cpu()\n",
    "    clf_ensemble = clf_ensemble.cpu()\n",
    "    clf_a = torch.sigmoid(clf_a).cpu()\n",
    "    clf_b = torch.sigmoid(clf_b).cpu()\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(num_samples, 5, figsize=(20, 4*num_samples))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Original image\n",
    "        axes[i, 0].imshow(images[i, 0], cmap='gray')\n",
    "        axes[i, 0].set_title(f'Input\\nTrue Label: {clf_labels[i].item()}')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Ground truth\n",
    "        axes[i, 1].imshow(masks[i, 0], cmap='gray')\n",
    "        axes[i, 1].set_title('Ground Truth Mask')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Network A\n",
    "        axes[i, 2].imshow(seg_a[i, 0], cmap='gray')\n",
    "        axes[i, 2].set_title(f'Net A\\nClf: {clf_a[i].item():.3f}')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        # Network B\n",
    "        axes[i, 3].imshow(seg_b[i, 0], cmap='gray')\n",
    "        axes[i, 3].set_title(f'Net B\\nClf: {clf_b[i].item():.3f}')\n",
    "        axes[i, 3].axis('off')\n",
    "        \n",
    "        # Ensemble\n",
    "        axes[i, 4].imshow(seg_ensemble[i, 0], cmap='gray')\n",
    "        axes[i, 4].set_title(f'Ensemble\\nClf: {clf_ensemble[i].item():.3f}')\n",
    "        axes[i, 4].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "evaluate_sample(codeepnet_trainer, train_loader, num_samples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef43dbc",
   "metadata": {},
   "source": [
    "## 9b. Comprehensive Model Performance Evaluation\n",
    "\n",
    "This section computes all essential metrics for medical image segmentation and classification:\n",
    "- **Classification Metrics**: Confusion matrix, accuracy, precision, recall, F1-score, ROC-AUC\n",
    "- **Segmentation Metrics**: IoU (Jaccard), Dice coefficient, pixel accuracy, sensitivity, specificity\n",
    "- **Per-Network Comparison**: Individual performance of Network A, Network B, and Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b155ba",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è IMPORTANT: Training Required!\n",
    "\n",
    "**Before running evaluation, you MUST complete training first!**\n",
    "\n",
    "### Proper Execution Order:\n",
    "1. ‚úÖ Run Section 1-6: Setup, architecture, dataset loading, model initialization\n",
    "2. ‚úÖ **Run Section 7: Training Loop** (20 epochs - this will take time!)\n",
    "3. ‚úÖ Run Section 8: Visualization of training dynamics\n",
    "4. ‚úÖ Run Section 9: Sample predictions\n",
    "5. ‚úÖ **Then run Section 9b: Performance evaluation** (this section)\n",
    "\n",
    "**If you see all zeros in metrics**, it means the model hasn't been trained yet. Go back to Section 7 and run the training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4054fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model has been trained\n",
    "print(\"üîç Checking Training Status...\\n\")\n",
    "\n",
    "training_status = codeepnet_trainer.get_training_summary()\n",
    "\n",
    "if training_status['total_batches'] == 0:\n",
    "    print(\"‚ùå MODEL NOT TRAINED YET!\")\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: You must run the training loop (Section 7) first!\")\n",
    "    print(\"   The model currently has random weights and will produce meaningless predictions.\\n\")\n",
    "    print(\"üìù Action Required:\")\n",
    "    print(\"   1. Scroll up to Section 7: Training Loop\")\n",
    "    print(\"   2. Run the training cell (it will take several minutes)\")\n",
    "    print(\"   3. Wait for training to complete (20 epochs)\")\n",
    "    print(\"   4. Then come back here and run the evaluation\\n\")\n",
    "    raise RuntimeError(\"Training required before evaluation. Please run Section 7 first.\")\n",
    "else:\n",
    "    print(\"‚úÖ MODEL HAS BEEN TRAINED!\")\n",
    "    print(f\"   ‚Ä¢ Total training batches: {training_status['total_batches']}\")\n",
    "    print(f\"   ‚Ä¢ Network swaps: {training_status['total_swaps']}\")\n",
    "    print(f\"   ‚Ä¢ Knowledge transmissions: {training_status['total_transmissions']}\")\n",
    "    print(f\"   ‚Ä¢ Network A avg loss: {training_status['avg_loss_a']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Network B avg loss: {training_status['avg_loss_b']:.4f}\")\n",
    "    print(\"\\n‚úÖ Ready to evaluate! Continue running cells below.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53b88be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "import seaborn as sns\n",
    "\n",
    "def compute_segmentation_metrics(pred_masks, true_masks, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute comprehensive segmentation metrics.\n",
    "    \n",
    "    Args:\n",
    "        pred_masks: Predicted probability maps (N, 1, H, W)\n",
    "        true_masks: Ground truth binary masks (N, 1, H, W)\n",
    "        threshold: Threshold for converting probabilities to binary\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with IoU, Dice, pixel accuracy, sensitivity, specificity\n",
    "    \"\"\"\n",
    "    # Convert to binary\n",
    "    pred_binary = (pred_masks > threshold).float()\n",
    "    true_binary = (true_masks > threshold).float()\n",
    "    \n",
    "    # Flatten for easier computation\n",
    "    pred_flat = pred_binary.view(-1)\n",
    "    true_flat = true_binary.view(-1)\n",
    "    \n",
    "    # True Positives, False Positives, False Negatives, True Negatives\n",
    "    TP = (pred_flat * true_flat).sum().item()\n",
    "    FP = (pred_flat * (1 - true_flat)).sum().item()\n",
    "    FN = ((1 - pred_flat) * true_flat).sum().item()\n",
    "    TN = ((1 - pred_flat) * (1 - true_flat)).sum().item()\n",
    "    \n",
    "    # Compute metrics\n",
    "    epsilon = 1e-7  # Avoid division by zero\n",
    "    \n",
    "    # Intersection over Union (IoU / Jaccard Index)\n",
    "    intersection = TP\n",
    "    union = TP + FP + FN\n",
    "    iou = intersection / (union + epsilon)\n",
    "    \n",
    "    # Dice Coefficient (F1-score for segmentation)\n",
    "    dice = (2 * TP) / (2 * TP + FP + FN + epsilon)\n",
    "    \n",
    "    # Pixel Accuracy\n",
    "    pixel_acc = (TP + TN) / (TP + TN + FP + FN + epsilon)\n",
    "    \n",
    "    # Sensitivity (Recall / True Positive Rate)\n",
    "    sensitivity = TP / (TP + FN + epsilon)\n",
    "    \n",
    "    # Specificity (True Negative Rate)\n",
    "    specificity = TN / (TN + FP + epsilon)\n",
    "    \n",
    "    # Precision\n",
    "    precision = TP / (TP + FP + epsilon)\n",
    "    \n",
    "    return {\n",
    "        'IoU': iou,\n",
    "        'Dice': dice,\n",
    "        'Pixel_Accuracy': pixel_acc,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "        'Precision': precision,\n",
    "        'TP': TP,\n",
    "        'FP': FP,\n",
    "        'FN': FN,\n",
    "        'TN': TN\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_classification_metrics(pred_probs, true_labels, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute classification metrics with confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        pred_probs: Predicted probabilities (N,)\n",
    "        true_labels: Ground truth labels (N,)\n",
    "        threshold: Threshold for binary classification\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with accuracy, precision, recall, F1, confusion matrix\n",
    "    \"\"\"\n",
    "    # Convert to numpy\n",
    "    pred_probs_np = pred_probs.cpu().numpy().flatten()\n",
    "    true_labels_np = true_labels.cpu().numpy().flatten()\n",
    "    \n",
    "    # Binary predictions\n",
    "    pred_labels = (pred_probs_np > threshold).astype(int)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(true_labels_np, pred_labels)\n",
    "    \n",
    "    # Compute metrics manually for clarity\n",
    "    TP = cm[1, 1] if cm.shape == (2, 2) else 0\n",
    "    FP = cm[0, 1] if cm.shape == (2, 2) else 0\n",
    "    FN = cm[1, 0] if cm.shape == (2, 2) else 0\n",
    "    TN = cm[0, 0] if cm.shape == (2, 2) else 0\n",
    "    \n",
    "    epsilon = 1e-7\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN + epsilon)\n",
    "    precision = TP / (TP + FP + epsilon)\n",
    "    recall = TP / (TP + FN + epsilon)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "    \n",
    "    # ROC-AUC (if both classes present)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(true_labels_np, pred_probs_np)\n",
    "    except:\n",
    "        roc_auc = 0.0\n",
    "    \n",
    "    return {\n",
    "        'Confusion_Matrix': cm,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1_Score': f1_score,\n",
    "        'ROC_AUC': roc_auc,\n",
    "        'TP': TP,\n",
    "        'FP': FP,\n",
    "        'FN': FN,\n",
    "        'TN': TN\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_full_dataset(trainer: CoDeepNetTrainer, dataloader: DataLoader, dataset_name='Train'):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation on full dataset.\n",
    "    \n",
    "    Computes metrics for:\n",
    "    - Network A (individual)\n",
    "    - Network B (individual)\n",
    "    - Ensemble (combined)\n",
    "    \n",
    "    For both segmentation and classification tasks.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üîç Evaluating on {dataset_name} Dataset ({len(dataloader.dataset)} samples)\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    trainer.unet_a.eval()\n",
    "    trainer.unet_b.eval()\n",
    "    \n",
    "    # Collect all predictions and labels\n",
    "    all_seg_preds_a = []\n",
    "    all_seg_preds_b = []\n",
    "    all_seg_preds_ensemble = []\n",
    "    all_seg_true = []\n",
    "    \n",
    "    all_clf_preds_a = []\n",
    "    all_clf_preds_b = []\n",
    "    all_clf_preds_ensemble = []\n",
    "    all_clf_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks, clf_labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images = images.to(trainer.device)\n",
    "            \n",
    "            # Get predictions from both networks\n",
    "            seg_a, clf_a = trainer.unet_a(images)\n",
    "            seg_b, clf_b = trainer.unet_b(images)\n",
    "            \n",
    "            # Ensemble (average)\n",
    "            seg_ensemble = (seg_a + seg_b) / 2\n",
    "            clf_ensemble = (clf_a + clf_b) / 2\n",
    "            \n",
    "            # Apply sigmoid to get probabilities\n",
    "            seg_a_prob = torch.sigmoid(seg_a)\n",
    "            seg_b_prob = torch.sigmoid(seg_b)\n",
    "            seg_ensemble_prob = torch.sigmoid(seg_ensemble)\n",
    "            \n",
    "            clf_a_prob = torch.sigmoid(clf_a)\n",
    "            clf_b_prob = torch.sigmoid(clf_b)\n",
    "            clf_ensemble_prob = torch.sigmoid(clf_ensemble)\n",
    "            \n",
    "            # Store predictions\n",
    "            all_seg_preds_a.append(seg_a_prob.cpu())\n",
    "            all_seg_preds_b.append(seg_b_prob.cpu())\n",
    "            all_seg_preds_ensemble.append(seg_ensemble_prob.cpu())\n",
    "            all_seg_true.append(masks.cpu())\n",
    "            \n",
    "            all_clf_preds_a.append(clf_a_prob.cpu())\n",
    "            all_clf_preds_b.append(clf_b_prob.cpu())\n",
    "            all_clf_preds_ensemble.append(clf_ensemble_prob.cpu())\n",
    "            all_clf_true.append(clf_labels.cpu())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    seg_preds_a = torch.cat(all_seg_preds_a, dim=0)\n",
    "    seg_preds_b = torch.cat(all_seg_preds_b, dim=0)\n",
    "    seg_preds_ensemble = torch.cat(all_seg_preds_ensemble, dim=0)\n",
    "    seg_true = torch.cat(all_seg_true, dim=0)\n",
    "    \n",
    "    clf_preds_a = torch.cat(all_clf_preds_a, dim=0)\n",
    "    clf_preds_b = torch.cat(all_clf_preds_b, dim=0)\n",
    "    clf_preds_ensemble = torch.cat(all_clf_preds_ensemble, dim=0)\n",
    "    clf_true = torch.cat(all_clf_true, dim=0)\n",
    "    \n",
    "    # =========================================\n",
    "    # SEGMENTATION METRICS\n",
    "    # =========================================\n",
    "    print(\"\\n\" + \"üéØ SEGMENTATION METRICS \".center(70, \"=\"))\n",
    "    \n",
    "    seg_metrics_a = compute_segmentation_metrics(seg_preds_a, seg_true)\n",
    "    seg_metrics_b = compute_segmentation_metrics(seg_preds_b, seg_true)\n",
    "    seg_metrics_ensemble = compute_segmentation_metrics(seg_preds_ensemble, seg_true)\n",
    "    \n",
    "    # Print segmentation metrics comparison\n",
    "    print(f\"\\n{'Metric':<20} {'Network A':>15} {'Network B':>15} {'Ensemble':>15}\")\n",
    "    print(\"-\" * 70)\n",
    "    for metric in ['IoU', 'Dice', 'Pixel_Accuracy', 'Sensitivity', 'Specificity', 'Precision']:\n",
    "        print(f\"{metric:<20} {seg_metrics_a[metric]:>15.4f} {seg_metrics_b[metric]:>15.4f} {seg_metrics_ensemble[metric]:>15.4f}\")\n",
    "    \n",
    "    # =========================================\n",
    "    # CLASSIFICATION METRICS\n",
    "    # =========================================\n",
    "    print(\"\\n\" + \"üéØ CLASSIFICATION METRICS \".center(70, \"=\"))\n",
    "    \n",
    "    clf_metrics_a = compute_classification_metrics(clf_preds_a, clf_true)\n",
    "    clf_metrics_b = compute_classification_metrics(clf_preds_b, clf_true)\n",
    "    clf_metrics_ensemble = compute_classification_metrics(clf_preds_ensemble, clf_true)\n",
    "    \n",
    "    # Print classification metrics comparison\n",
    "    print(f\"\\n{'Metric':<20} {'Network A':>15} {'Network B':>15} {'Ensemble':>15}\")\n",
    "    print(\"-\" * 70)\n",
    "    for metric in ['Accuracy', 'Precision', 'Recall', 'F1_Score', 'ROC_AUC']:\n",
    "        print(f\"{metric:<20} {clf_metrics_a[metric]:>15.4f} {clf_metrics_b[metric]:>15.4f} {clf_metrics_ensemble[metric]:>15.4f}\")\n",
    "    \n",
    "    # =========================================\n",
    "    # CONFUSION MATRICES\n",
    "    # =========================================\n",
    "    print(\"\\n\" + \"üìä CONFUSION MATRICES \".center(70, \"=\"))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Network A confusion matrix\n",
    "    sns.heatmap(clf_metrics_a['Confusion_Matrix'], annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "                xticklabels=['No Tumor', 'Tumor'], yticklabels=['No Tumor', 'Tumor'])\n",
    "    axes[0].set_title(f\"Network A\\nAccuracy: {clf_metrics_a['Accuracy']:.3f}\", fontsize=12)\n",
    "    axes[0].set_ylabel('True Label')\n",
    "    axes[0].set_xlabel('Predicted Label')\n",
    "    \n",
    "    # Network B confusion matrix\n",
    "    sns.heatmap(clf_metrics_b['Confusion_Matrix'], annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "                xticklabels=['No Tumor', 'Tumor'], yticklabels=['No Tumor', 'Tumor'])\n",
    "    axes[1].set_title(f\"Network B\\nAccuracy: {clf_metrics_b['Accuracy']:.3f}\", fontsize=12)\n",
    "    axes[1].set_ylabel('True Label')\n",
    "    axes[1].set_xlabel('Predicted Label')\n",
    "    \n",
    "    # Ensemble confusion matrix\n",
    "    sns.heatmap(clf_metrics_ensemble['Confusion_Matrix'], annot=True, fmt='d', cmap='Oranges', ax=axes[2],\n",
    "                xticklabels=['No Tumor', 'Tumor'], yticklabels=['No Tumor', 'Tumor'])\n",
    "    axes[2].set_title(f\"Ensemble\\nAccuracy: {clf_metrics_ensemble['Accuracy']:.3f}\", fontsize=12)\n",
    "    axes[2].set_ylabel('True Label')\n",
    "    axes[2].set_xlabel('Predicted Label')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # =========================================\n",
    "    # ROC CURVES\n",
    "    # =========================================\n",
    "    print(\"\\n\" + \"üìà ROC CURVES \".center(70, \"=\"))\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Compute ROC curves\n",
    "    clf_true_np = clf_true.numpy().flatten()\n",
    "    \n",
    "    if len(np.unique(clf_true_np)) > 1:  # Only if both classes present\n",
    "        fpr_a, tpr_a, _ = roc_curve(clf_true_np, clf_preds_a.numpy().flatten())\n",
    "        fpr_b, tpr_b, _ = roc_curve(clf_true_np, clf_preds_b.numpy().flatten())\n",
    "        fpr_ens, tpr_ens, _ = roc_curve(clf_true_np, clf_preds_ensemble.numpy().flatten())\n",
    "        \n",
    "        plt.plot(fpr_a, tpr_a, label=f'Network A (AUC = {clf_metrics_a[\"ROC_AUC\"]:.3f})', linewidth=2)\n",
    "        plt.plot(fpr_b, tpr_b, label=f'Network B (AUC = {clf_metrics_b[\"ROC_AUC\"]:.3f})', linewidth=2)\n",
    "        plt.plot(fpr_ens, tpr_ens, label=f'Ensemble (AUC = {clf_metrics_ensemble[\"ROC_AUC\"]:.3f})', linewidth=2, linestyle='--')\n",
    "        plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', alpha=0.3)\n",
    "        \n",
    "        plt.xlabel('False Positive Rate', fontsize=12)\n",
    "        plt.ylabel('True Positive Rate', fontsize=12)\n",
    "        plt.title('ROC Curves: Tumor Classification', fontsize=14, fontweight='bold')\n",
    "        plt.legend(loc='lower right', fontsize=11)\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è ROC curve requires both classes in the dataset\")\n",
    "    \n",
    "    # =========================================\n",
    "    # SUMMARY REPORT\n",
    "    # =========================================\n",
    "    print(\"\\n\" + \"üìã PERFORMANCE SUMMARY \".center(70, \"=\"))\n",
    "    print(f\"\\nüèÜ Best Segmentation (Dice): \", end=\"\")\n",
    "    best_dice = max(seg_metrics_a['Dice'], seg_metrics_b['Dice'], seg_metrics_ensemble['Dice'])\n",
    "    if seg_metrics_ensemble['Dice'] == best_dice:\n",
    "        print(f\"Ensemble ({seg_metrics_ensemble['Dice']:.4f})\")\n",
    "    elif seg_metrics_a['Dice'] == best_dice:\n",
    "        print(f\"Network A ({seg_metrics_a['Dice']:.4f})\")\n",
    "    else:\n",
    "        print(f\"Network B ({seg_metrics_b['Dice']:.4f})\")\n",
    "    \n",
    "    print(f\"üèÜ Best Classification (F1): \", end=\"\")\n",
    "    best_f1 = max(clf_metrics_a['F1_Score'], clf_metrics_b['F1_Score'], clf_metrics_ensemble['F1_Score'])\n",
    "    if clf_metrics_ensemble['F1_Score'] == best_f1:\n",
    "        print(f\"Ensemble ({clf_metrics_ensemble['F1_Score']:.4f})\")\n",
    "    elif clf_metrics_a['F1_Score'] == best_f1:\n",
    "        print(f\"Network A ({clf_metrics_a['F1_Score']:.4f})\")\n",
    "    else:\n",
    "        print(f\"Network B ({clf_metrics_b['F1_Score']:.4f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return {\n",
    "        'segmentation': {\n",
    "            'network_a': seg_metrics_a,\n",
    "            'network_b': seg_metrics_b,\n",
    "            'ensemble': seg_metrics_ensemble\n",
    "        },\n",
    "        'classification': {\n",
    "            'network_a': clf_metrics_a,\n",
    "            'network_b': clf_metrics_b,\n",
    "            'ensemble': clf_metrics_ensemble\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "evaluation_results = evaluate_full_dataset(codeepnet_trainer, train_loader, dataset_name='Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1f6308",
   "metadata": {},
   "source": [
    "## 9c. Detailed Performance Analysis & Error Cases\n",
    "\n",
    "Analyze where the model performs well and where it struggles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fdf4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_best_and_worst_cases(trainer: CoDeepNetTrainer, dataloader: DataLoader, num_samples=3):\n",
    "    \"\"\"\n",
    "    Visualize best and worst predictions to understand model behavior.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"üîç QUALITATIVE ANALYSIS \".center(70, \"=\"))\n",
    "    print(\"\\nFinding best and worst predictions...\\n\")\n",
    "    \n",
    "    trainer.unet_a.eval()\n",
    "    trainer.unet_b.eval()\n",
    "    \n",
    "    # Collect all samples with their metrics\n",
    "    all_samples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks, clf_labels in dataloader:\n",
    "            images_device = images.to(trainer.device)\n",
    "            \n",
    "            # Get ensemble predictions\n",
    "            seg_ensemble, clf_ensemble, _, _ = trainer.predict_ensemble(images_device)\n",
    "            \n",
    "            # Compute per-sample Dice scores\n",
    "            for i in range(images.size(0)):\n",
    "                seg_pred = seg_ensemble[i:i+1].cpu()\n",
    "                seg_true = masks[i:i+1]\n",
    "                \n",
    "                # Compute Dice score\n",
    "                pred_binary = (seg_pred > 0.5).float()\n",
    "                true_binary = (seg_true > 0.5).float()\n",
    "                \n",
    "                intersection = (pred_binary * true_binary).sum().item()\n",
    "                union = pred_binary.sum().item() + true_binary.sum().item()\n",
    "                dice = (2 * intersection) / (union + 1e-7)\n",
    "                \n",
    "                # Classification correctness\n",
    "                clf_pred_class = (clf_ensemble[i].cpu() > 0.5).long().item()\n",
    "                clf_true_class = clf_labels[i].item()\n",
    "                clf_correct = (clf_pred_class == clf_true_class)\n",
    "                \n",
    "                all_samples.append({\n",
    "                    'image': images[i],\n",
    "                    'mask': masks[i],\n",
    "                    'seg_pred': seg_pred[0],\n",
    "                    'clf_pred': clf_ensemble[i].cpu().item(),\n",
    "                    'clf_true': clf_true_class,\n",
    "                    'dice': dice,\n",
    "                    'clf_correct': clf_correct\n",
    "                })\n",
    "    \n",
    "    # Sort by Dice score\n",
    "    all_samples.sort(key=lambda x: x['dice'], reverse=True)\n",
    "    \n",
    "    # Get best and worst cases\n",
    "    best_samples = all_samples[:num_samples]\n",
    "    worst_samples = all_samples[-num_samples:]\n",
    "    \n",
    "    # Visualize best cases\n",
    "    print(f\"‚úÖ TOP {num_samples} BEST PREDICTIONS (Highest Dice Scores)\")\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, sample in enumerate(best_samples):\n",
    "        axes[i, 0].imshow(sample['image'][0], cmap='gray')\n",
    "        axes[i, 0].set_title(f\"Input Image\\nTrue: {'Tumor' if sample['clf_true'] else 'Healthy'}\", fontsize=10)\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(sample['mask'][0], cmap='gray')\n",
    "        axes[i, 1].set_title('Ground Truth Mask', fontsize=10)\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(sample['seg_pred'][0], cmap='gray')\n",
    "        axes[i, 2].set_title(f\"Prediction\\nDice: {sample['dice']:.4f}\\nClf: {sample['clf_pred']:.3f} {'‚úì' if sample['clf_correct'] else '‚úó'}\", fontsize=10)\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize worst cases\n",
    "    print(f\"\\n‚ùå TOP {num_samples} WORST PREDICTIONS (Lowest Dice Scores)\")\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, sample in enumerate(worst_samples):\n",
    "        axes[i, 0].imshow(sample['image'][0], cmap='gray')\n",
    "        axes[i, 0].set_title(f\"Input Image\\nTrue: {'Tumor' if sample['clf_true'] else 'Healthy'}\", fontsize=10)\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(sample['mask'][0], cmap='gray')\n",
    "        axes[i, 1].set_title('Ground Truth Mask', fontsize=10)\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(sample['seg_pred'][0], cmap='gray')\n",
    "        axes[i, 2].set_title(f\"Prediction\\nDice: {sample['dice']:.4f}\\nClf: {sample['clf_pred']:.3f} {'‚úì' if sample['clf_correct'] else '‚úó'}\", fontsize=10)\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    dice_scores = [s['dice'] for s in all_samples]\n",
    "    clf_accuracy = sum([s['clf_correct'] for s in all_samples]) / len(all_samples)\n",
    "    \n",
    "    print(f\"\\nüìä STATISTICS:\")\n",
    "    print(f\"  Mean Dice Score: {np.mean(dice_scores):.4f} ¬± {np.std(dice_scores):.4f}\")\n",
    "    print(f\"  Median Dice Score: {np.median(dice_scores):.4f}\")\n",
    "    print(f\"  Min Dice Score: {np.min(dice_scores):.4f}\")\n",
    "    print(f\"  Max Dice Score: {np.max(dice_scores):.4f}\")\n",
    "    print(f\"  Classification Accuracy: {clf_accuracy:.4f} ({int(clf_accuracy*len(all_samples))}/{len(all_samples)})\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "# Run qualitative analysis\n",
    "analyze_best_and_worst_cases(codeepnet_trainer, train_loader, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c18f21",
   "metadata": {},
   "source": [
    "## 9d. Generate Performance Report\n",
    "\n",
    "Create a comprehensive performance report for your project submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935f732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def generate_performance_report(evaluation_results, trainer, save_path=None):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive performance report for project documentation.\n",
    "    \"\"\"\n",
    "    seg_metrics = evaluation_results['segmentation']\n",
    "    clf_metrics = evaluation_results['classification']\n",
    "    \n",
    "    report = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'model': 'Co-DeepNet U-Net',\n",
    "        'architecture': {\n",
    "            'description': 'Cooperative learning with two U-Nets',\n",
    "            'tag_team_training': True,\n",
    "            'knowledge_transmission': True,\n",
    "            'swap_frequency': trainer.swap_frequency,\n",
    "            'transmission_frequency': trainer.transmission_frequency,\n",
    "            'transmission_rate': trainer.transmitter.transmission_rate,\n",
    "            'transmission_strategy': trainer.transmitter.strategy\n",
    "        },\n",
    "        'training_summary': trainer.get_training_summary(),\n",
    "        'segmentation_performance': {\n",
    "            'ensemble': {\n",
    "                'IoU': float(seg_metrics['ensemble']['IoU']),\n",
    "                'Dice': float(seg_metrics['ensemble']['Dice']),\n",
    "                'Pixel_Accuracy': float(seg_metrics['ensemble']['Pixel_Accuracy']),\n",
    "                'Sensitivity': float(seg_metrics['ensemble']['Sensitivity']),\n",
    "                'Specificity': float(seg_metrics['ensemble']['Specificity']),\n",
    "                'Precision': float(seg_metrics['ensemble']['Precision'])\n",
    "            },\n",
    "            'network_a': {\n",
    "                'IoU': float(seg_metrics['network_a']['IoU']),\n",
    "                'Dice': float(seg_metrics['network_a']['Dice'])\n",
    "            },\n",
    "            'network_b': {\n",
    "                'IoU': float(seg_metrics['network_b']['IoU']),\n",
    "                'Dice': float(seg_metrics['network_b']['Dice'])\n",
    "            }\n",
    "        },\n",
    "        'classification_performance': {\n",
    "            'ensemble': {\n",
    "                'Accuracy': float(clf_metrics['ensemble']['Accuracy']),\n",
    "                'Precision': float(clf_metrics['ensemble']['Precision']),\n",
    "                'Recall': float(clf_metrics['ensemble']['Recall']),\n",
    "                'F1_Score': float(clf_metrics['ensemble']['F1_Score']),\n",
    "                'ROC_AUC': float(clf_metrics['ensemble']['ROC_AUC']),\n",
    "                'Confusion_Matrix': clf_metrics['ensemble']['Confusion_Matrix'].tolist()\n",
    "            },\n",
    "            'network_a': {\n",
    "                'Accuracy': float(clf_metrics['network_a']['Accuracy']),\n",
    "                'F1_Score': float(clf_metrics['network_a']['F1_Score']),\n",
    "                'ROC_AUC': float(clf_metrics['network_a']['ROC_AUC'])\n",
    "            },\n",
    "            'network_b': {\n",
    "                'Accuracy': float(clf_metrics['network_b']['Accuracy']),\n",
    "                'F1_Score': float(clf_metrics['network_b']['F1_Score']),\n",
    "                'ROC_AUC': float(clf_metrics['network_b']['ROC_AUC'])\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Print formatted report\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä CO-DEEPNET U-NET PERFORMANCE REPORT\".center(80))\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüìÖ Generated: {report['timestamp']}\")\n",
    "    print(f\"üèóÔ∏è  Architecture: {report['model']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"TRAINING CONFIGURATION \".center(80, \"-\"))\n",
    "    print(f\"  ‚Ä¢ Swap Frequency: {report['architecture']['swap_frequency']} batches\")\n",
    "    print(f\"  ‚Ä¢ Transmission Frequency: {report['architecture']['transmission_frequency']} batches\")\n",
    "    print(f\"  ‚Ä¢ Transmission Rate: {report['architecture']['transmission_rate']}\")\n",
    "    print(f\"  ‚Ä¢ Transmission Strategy: {report['architecture']['transmission_strategy']}\")\n",
    "    print(f\"  ‚Ä¢ Total Batches: {report['training_summary']['total_batches']}\")\n",
    "    print(f\"  ‚Ä¢ Total Swaps: {report['training_summary']['total_swaps']}\")\n",
    "    print(f\"  ‚Ä¢ Total Transmissions: {report['training_summary']['total_transmissions']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"SEGMENTATION PERFORMANCE \".center(80, \"-\"))\n",
    "    ens_seg = report['segmentation_performance']['ensemble']\n",
    "    print(f\"  üìà IoU (Intersection over Union): {ens_seg['IoU']:.4f}\")\n",
    "    print(f\"  üìà Dice Coefficient: {ens_seg['Dice']:.4f}\")\n",
    "    print(f\"  üìà Pixel Accuracy: {ens_seg['Pixel_Accuracy']:.4f}\")\n",
    "    print(f\"  üìà Sensitivity (Recall): {ens_seg['Sensitivity']:.4f}\")\n",
    "    print(f\"  üìà Specificity: {ens_seg['Specificity']:.4f}\")\n",
    "    print(f\"  üìà Precision: {ens_seg['Precision']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"CLASSIFICATION PERFORMANCE \".center(80, \"-\"))\n",
    "    ens_clf = report['classification_performance']['ensemble']\n",
    "    print(f\"  üìä Accuracy: {ens_clf['Accuracy']:.4f}\")\n",
    "    print(f\"  üìä Precision: {ens_clf['Precision']:.4f}\")\n",
    "    print(f\"  üìä Recall: {ens_clf['Recall']:.4f}\")\n",
    "    print(f\"  üìä F1-Score: {ens_clf['F1_Score']:.4f}\")\n",
    "    print(f\"  üìä ROC-AUC: {ens_clf['ROC_AUC']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"CONFUSION MATRIX (ENSEMBLE) \".center(80, \"-\"))\n",
    "    cm = np.array(ens_clf['Confusion_Matrix'])\n",
    "    print(f\"                    Predicted\")\n",
    "    print(f\"                No Tumor  |  Tumor\")\n",
    "    print(f\"  Actual  No Tumor   {cm[0,0]:>4}    |  {cm[0,1]:>4}\")\n",
    "    print(f\"          Tumor      {cm[1,0]:>4}    |  {cm[1,1]:>4}\")\n",
    "    \n",
    "    print(\"\\n\" + \"NETWORK COMPARISON \".center(80, \"-\"))\n",
    "    print(f\"  {'Metric':<20} {'Net A':>12} {'Net B':>12} {'Ensemble':>12} {'Winner':>12}\")\n",
    "    print(\"  \" + \"-\"*76)\n",
    "    \n",
    "    # Segmentation comparison\n",
    "    dice_a = report['segmentation_performance']['network_a']['Dice']\n",
    "    dice_b = report['segmentation_performance']['network_b']['Dice']\n",
    "    dice_ens = report['segmentation_performance']['ensemble']['Dice']\n",
    "    winner = 'Ensemble' if dice_ens >= max(dice_a, dice_b) else ('Net A' if dice_a > dice_b else 'Net B')\n",
    "    print(f\"  {'Dice Score':<20} {dice_a:>12.4f} {dice_b:>12.4f} {dice_ens:>12.4f} {winner:>12}\")\n",
    "    \n",
    "    # Classification comparison\n",
    "    f1_a = report['classification_performance']['network_a']['F1_Score']\n",
    "    f1_b = report['classification_performance']['network_b']['F1_Score']\n",
    "    f1_ens = report['classification_performance']['ensemble']['F1_Score']\n",
    "    winner = 'Ensemble' if f1_ens >= max(f1_a, f1_b) else ('Net A' if f1_a > f1_b else 'Net B')\n",
    "    print(f\"  {'F1-Score':<20} {f1_a:>12.4f} {f1_b:>12.4f} {f1_ens:>12.4f} {winner:>12}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ ENSEMBLE CONSISTENTLY OUTPERFORMS INDIVIDUAL NETWORKS\" if \n",
    "          dice_ens >= max(dice_a, dice_b) and f1_ens >= max(f1_a, f1_b)\n",
    "          else \"‚ö†Ô∏è  MIXED RESULTS - REVIEW INDIVIDUAL NETWORK PERFORMANCE\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Save to file if path provided\n",
    "    if save_path:\n",
    "        save_path = Path(save_path)\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        \n",
    "        print(f\"üíæ Report saved to: {save_path}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "\n",
    "# Generate and save report\n",
    "report_path = Path('/work/IdaHayJ√∏rgensen#9284/Notebooks/models/performance_report.json')\n",
    "performance_report = generate_performance_report(\n",
    "    evaluation_results, \n",
    "    codeepnet_trainer,\n",
    "    save_path=report_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f206b3",
   "metadata": {},
   "source": [
    "## üìö Understanding the Metrics\n",
    "\n",
    "### Segmentation Metrics (Pixel-level tumor localization)\n",
    "\n",
    "| Metric | Description | Range | Interpretation |\n",
    "|--------|-------------|-------|----------------|\n",
    "| **IoU** (Jaccard Index) | Intersection over Union of predicted and true masks | 0-1 | >0.7 = good, >0.5 = acceptable |\n",
    "| **Dice Coefficient** | F1-score for segmentation, more weight on overlap | 0-1 | >0.8 = excellent, >0.7 = good |\n",
    "| **Pixel Accuracy** | Percentage of correctly classified pixels | 0-1 | Can be misleading with class imbalance |\n",
    "| **Sensitivity** | True Positive Rate (tumor pixels detected) | 0-1 | High = good at finding tumors |\n",
    "| **Specificity** | True Negative Rate (healthy pixels correct) | 0-1 | High = good at avoiding false alarms |\n",
    "| **Precision** | Proportion of predicted tumor that is actually tumor | 0-1 | High = fewer false positives |\n",
    "\n",
    "**Key Insight**: Dice and IoU are most important for medical segmentation. Dice > 0.7 is considered clinically useful.\n",
    "\n",
    "---\n",
    "\n",
    "### Classification Metrics (Tumor present or not?)\n",
    "\n",
    "| Metric | Description | Range | Interpretation |\n",
    "|--------|-------------|-------|----------------|\n",
    "| **Accuracy** | Overall correctness (TP+TN)/(TP+TN+FP+FN) | 0-1 | Simple but can be misleading |\n",
    "| **Precision** | TP/(TP+FP) - When model says tumor, is it correct? | 0-1 | High = trust positive predictions |\n",
    "| **Recall** | TP/(TP+FN) - Of all actual tumors, how many found? | 0-1 | High = doesn't miss tumors |\n",
    "| **F1-Score** | Harmonic mean of Precision and Recall | 0-1 | Balanced metric, >0.8 = good |\n",
    "| **ROC-AUC** | Area Under Receiver Operating Characteristic | 0-1 | >0.9 = excellent discrimination |\n",
    "\n",
    "**Key Insight**: F1-Score balances precision and recall. ROC-AUC shows model's ability to separate classes.\n",
    "\n",
    "---\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "```\n",
    "                    Predicted\n",
    "             No Tumor  |  Tumor\n",
    "Actual  No    TN      |   FP      (False Positive = false alarm)\n",
    "        Tumor FN      |   TP      (False Negative = missed tumor)\n",
    "```\n",
    "\n",
    "- **TN (True Negative)**: Correctly identified healthy patients\n",
    "- **TP (True Positive)**: Correctly identified tumor patients\n",
    "- **FP (False Positive)**: Incorrectly flagged healthy as tumor (Type I error)\n",
    "- **FN (False Negative)**: Missed actual tumor (Type II error) - **MOST DANGEROUS**\n",
    "\n",
    "**Medical Context**: In cancer detection, False Negatives (missing tumors) are more dangerous than False Positives (false alarms), so we prioritize **high Recall/Sensitivity**.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Ensemble Works Better\n",
    "\n",
    "The Co-DeepNet ensemble combines predictions from two networks that:\n",
    "1. Started with different random initializations\n",
    "2. Explored different solution spaces during tag-team training\n",
    "3. Learned complementary patterns through knowledge transmission\n",
    "4. Reduce individual network weaknesses through averaging\n",
    "\n",
    "**Expected Result**: Ensemble should have:\n",
    "- Higher Dice/IoU (better segmentation)\n",
    "- Higher F1/ROC-AUC (better classification)\n",
    "- More stable predictions (lower variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1816f20",
   "metadata": {},
   "source": [
    "## 10. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839720c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save both networks and training state\n",
    "# Use virtual environment path\n",
    "save_dir = Path('/work/IdaHayJ√∏rgensen#9284/Notebooks/models')\n",
    "save_dir.mkdir(parents=True, exist_ok=True)  # parents=True creates all parent directories\n",
    "\n",
    "torch.save({\n",
    "    'unet_a_state': codeepnet_trainer.unet_a.state_dict(),\n",
    "    'unet_b_state': codeepnet_trainer.unet_b.state_dict(),\n",
    "    'optimizer_a_state': codeepnet_trainer.optimizer_a.state_dict(),\n",
    "    'optimizer_b_state': codeepnet_trainer.optimizer_b.state_dict(),\n",
    "    'training_history': codeepnet_trainer.history,\n",
    "    'config': {\n",
    "        'swap_frequency': codeepnet_trainer.swap_frequency,\n",
    "        'transmission_frequency': codeepnet_trainer.transmission_frequency,\n",
    "        'transmission_rate': codeepnet_trainer.transmitter.transmission_rate,\n",
    "        'transmission_strategy': codeepnet_trainer.transmitter.strategy\n",
    "    }\n",
    "}, save_dir / 'codeepnet_unet_checkpoint.pth')\n",
    "\n",
    "print(f\"‚úì Models saved to {save_dir / 'codeepnet_unet_checkpoint.pth'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac382f",
   "metadata": {},
   "source": [
    "## 11. Analysis: Why Co-DeepNet Works\n",
    "\n",
    "### Key Insights from Training:\n",
    "\n",
    "1. **Diverse Exploration**: Two networks initialized differently explore distinct regions of the solution space\n",
    "2. **Guided Learning**: Knowledge transmission allows networks to benefit from each other's discoveries\n",
    "3. **Reduced Overfitting**: Tag-team training prevents both networks from overfitting to the same patterns\n",
    "4. **Computational Efficiency**: Only one network trains at a time, using less GPU memory than training both simultaneously\n",
    "\n",
    "### Comparison to Original Paper:\n",
    "\n",
    "| Aspect | Co-DeepNet Paper | This Implementation |\n",
    "|--------|------------------|---------------------|\n",
    "| Task | Age prediction | Tumor classification + segmentation |\n",
    "| Architecture | CNNs | U-Nets |\n",
    "| Training | Tag-team | Tag-team ‚úì |\n",
    "| Knowledge Transfer | Periodic | Periodic ‚úì |\n",
    "| Optimization | Genetic algorithm | Adam (+ optional GA) |\n",
    "| Ensemble | Yes | Yes ‚úì |\n",
    "\n",
    "### Future Improvements:\n",
    "- Add genetic algorithm for architecture search\n",
    "- Implement dynamic transmission rate adjustment\n",
    "- Add validation set evaluation\n",
    "- Compare against single large U-Net baseline"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
